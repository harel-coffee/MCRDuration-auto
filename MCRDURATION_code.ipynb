{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46df8818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from os.path import exists\n",
    "import itertools\n",
    "import ast\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "import xgboost as xg\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "from sklearn.metrics import f1_score as F1\n",
    "from sklearn.metrics import matthews_corrcoef as MCC \n",
    "from imblearn.metrics import geometric_mean_score as Gm\n",
    "from sklearn.metrics import recall_score as recall\n",
    "from sklearn.metrics import precision_score as precision\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import time\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from yellowbrick.regressor import CooksDistance\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import plotly.express as px\n",
    "from scipy.stats import boxcox\n",
    "from scipy.special import inv_boxcox\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.special import inv_boxcox1p\n",
    "from flaml import AutoML\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "from sklearn.linear_model import HuberRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e385ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORGANIZATION = 'Eclipse'\n",
    "DATA_PATH = './datasets'\n",
    "REGRESSION_VARIABLE = 'date_updated_date_created_diff'\n",
    "N_REPITIONS = 100\n",
    "VALIDATION_SCHEMA = TimeSeriesSplit(n_splits=10)\n",
    "REGRESSOR_MODEL = RandomForestRegressor()\n",
    "\n",
    "# class for random guessing \n",
    "class RandomGuess: \n",
    "    def __init__(self): \n",
    "        self.X =None\n",
    "        self.y = None\n",
    "    \n",
    "    def fit(self,X,y) : \n",
    "        self.X = X \n",
    "        self.y = list(y)\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return [random.choice(self.y) for _ in range(len(X))]\n",
    "    \n",
    "MODELS = {\n",
    "    'ExtraTrees' : ExtraTreesRegressor(),\n",
    "    'Random_forest' : RandomForestRegressor(), \n",
    "    'LightGBM' : lgb.LGBMRegressor(),\n",
    "    'Decision_tree' : DecisionTreeRegressor(max_depth=10),\n",
    "    'XGBOOST' : xg.XGBRegressor(),\n",
    "    'AdaBoost' :AdaBoostRegressor(),\n",
    "    'Dummy_median' : DummyRegressor(strategy=\"median\"), \n",
    "    'Dummy_mean' : DummyRegressor(strategy= 'mean')\n",
    "    \n",
    "    \n",
    "    \n",
    "}\n",
    "ALL_MODELS = {\n",
    "    'ExtraTrees' : {\n",
    "        'default' : ExtraTreesRegressor(),\n",
    "        'grid' : {\n",
    "            'criterion' : ['absolute_error'],\n",
    "            'max_depth' : [5,10,None],\n",
    "            'n_estimators' : [100,500]\n",
    "        }\n",
    "    },\n",
    "    'Random_forest' : {\n",
    "        'default' :RandomForestRegressor(n_jobs = -1) , \n",
    "        'grid' : {\n",
    "            'criterion' : ['squared_error', 'poisson'],\n",
    "            'max_depth' : [5,10,None],\n",
    "            'n_estimators' : [100,500]\n",
    "        }\n",
    "    },\n",
    "    'Decision_tree': {\n",
    "        'default' :DecisionTreeRegressor() , \n",
    "        'grid' :{\n",
    "            'criterion' : ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "            'max_depth' : [5,10,None],\n",
    "            'splitter' : ['best', 'random']\n",
    "        }\n",
    "    },\n",
    "    'XGBOOST' : {\n",
    "        'default' : xg.XGBRegressor(n_jobs = -1),\n",
    "        'grid' : {\n",
    "            'max_depth' : [5,10,None],\n",
    "            'n_estimators' : [100,500],\n",
    "            'learning_rate': [0.01,0.1,0.3]\n",
    "        }\n",
    "    },\n",
    "    'AdaBoost' : {\n",
    "        'default' :AdaBoostRegressor(),\n",
    "        'grid' : {\n",
    "            'n_estimators' : [100,500],\n",
    "            'learning_rate': [0.01,0.1,0.3],\n",
    "            'loss' :['linear', 'square', 'exponential']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "FEATURES =[\n",
    "    #Time\n",
    "    'created_weekday_utc',\n",
    "    'created_weekday_owner_tz',\n",
    "    'created_hours_utc',\n",
    "    'created_hours_owner_tz',\n",
    "     #collaboration graph\n",
    "     'degree_centrality',\n",
    "     'closeness_centrality',\n",
    "     'betweenness_centrality',\n",
    "     'eigenvector_centrality',\n",
    "     'clustering_coefficient',\n",
    "     'core_number',\n",
    "     #Change\n",
    "     'insertions',\n",
    "     'deletions',\n",
    "     'num_files',\n",
    "     'num_files_type',\n",
    "     'num_directory',\n",
    "     'num_programming_languages',\n",
    "     'modify_entropy',\n",
    "     'code_churn',\n",
    "     'sum_loc',\n",
    "     'sum_complexity',\n",
    "     'num_binary_files',\n",
    "      #text\n",
    "     'subject_length',\n",
    "     'subject_word_count',\n",
    "     'msg_length',\n",
    "     'msg_word_count',\n",
    "     'is_non_fonctional',\n",
    "     'is_refactoring',\n",
    "     'is_corrective',\n",
    "     'is_preventive',\n",
    "     'has_feature_addition',\n",
    "     'is_merge',\n",
    "     #Files\n",
    "     'avg_num_dev_modified_files',\n",
    "     'num_files_changes',\n",
    "     'num_files_changes_avg',\n",
    "     'files_changes_duration_avg',\n",
    "     'files_revisions_duration_avg',\n",
    "     'files_num_recent_branch_changes',\n",
    "     'num_file_changes_for_revrs_avg_recent',\n",
    "     'num_file_changes_for_revrs_max_recent',\n",
    "     'file_changes_time_revrs_avg_recent',\n",
    "     'file_changes_time_revrs_max_recent',\n",
    "     'num_owner_prior_changes',\n",
    "     'num_owner_open_changes_recent',\n",
    "      #Owner\n",
    "     'owner_age',\n",
    "     'prior_owner_rate',\n",
    "     'owner_changes_messages',\n",
    "     'owner_changes_messages_avg',\n",
    "     'owner_time_between_message_avg',\n",
    "     'owner_merged_ratio',\n",
    "     'num_prior_owner_project_changes',\n",
    "     'prior_owner_project_changes_ratio',\n",
    "     'owner_revrs_commons_changes_recent',\n",
    "     'owner_revrs_commons_msgs_avg_recent',\n",
    "     #history\n",
    "     'num_prior_changes',\n",
    "     'num_prior_project_changes',\n",
    "     'num_open_changes_recent',\n",
    "     'num_project_open_changes_recent',\n",
    "      #Reviewers \n",
    "     'revrs_changes_recent',\n",
    "     'revrs_changes_avg_recent',\n",
    "     'revrs_merged_changes_recent',\n",
    "     'revrs_merged_changes_avg_recent',\n",
    "     'revrs_abandoned_changes_recent',\n",
    "     'revrs_abandoned_changes_avg_recent',\n",
    "     'revrs_open_changes_recent',\n",
    "     'revrs_open_changes_avg_recent',\n",
    "     'revrs_num_review_recent',\n",
    "     'revrs_num_review_avg_recent',\n",
    "     'revrs_previous_msgs_recent',\n",
    "     'revrs_previous_msgs_avg_recent'\n",
    "]\n",
    "\n",
    "BINARY_VARIABLES = [\n",
    "    'is_non_fonctional','is_refactoring','is_corrective','is_preventive','is_author_core','is_master_branch'\n",
    "]\n",
    "MULTICATEGORICAL_FEATURES = [\n",
    "    'code_churn_size','created_weekday_utc','created_weekday_owner_tz'\n",
    "]\n",
    "ALL_CATEGORICAL_FEATURES = MULTICATEGORICAL_FEATURES + BINARY_VARIABLES\n",
    "\n",
    "DIMS = {\n",
    "    'all': FEATURES,\n",
    "    \n",
    "    'time':['created_weekday_utc','created_weekday_owner_tz','created_hours_utc',\n",
    "            'created_hours_owner_tz'],\n",
    "    \n",
    "    'change': ['insertions','deletions','num_files','num_files_type','num_directory',\n",
    "               'num_programming_languages','modify_entropy','code_churn','sum_loc',\n",
    "               'sum_complexity','num_binary_files'],\n",
    "    \n",
    "    'text': ['subject_length','subject_word_count','msg_length','msg_word_count',\n",
    "             'is_non_fonctional','is_refactoring','is_corrective','is_preventive',\n",
    "             'has_feature_addition','is_merge'],\n",
    "    \n",
    "    'files': ['avg_num_dev_modified_files','num_files_changes','num_files_changes_avg',\n",
    "              'files_changes_duration_avg','files_revisions_duration_avg',\n",
    "              'files_num_recent_branch_changes','num_file_changes_for_revrs_avg_recent',\n",
    "              'num_file_changes_for_revrs_max_recent','file_changes_time_revrs_avg_recent',\n",
    "              'file_changes_time_revrs_max_recent'],\n",
    "    \n",
    "    'Owner': ['num_owner_prior_changes','num_owner_open_changes_recent','owner_age',\n",
    "           'prior_owner_rate','owner_changes_messages','owner_changes_messages_avg',\n",
    "           'owner_time_between_message_avg','owner_merged_ratio','num_prior_owner_project_changes',\n",
    "           'prior_owner_project_changes_ratio'],\n",
    "    \n",
    "    'history': ['num_prior_changes','num_prior_project_changes','num_open_changes_recent',\n",
    "             'num_project_open_changes_recent'],\n",
    "    \n",
    "    'reviewers': ['revrs_changes_recent','revrs_changes_avg_recent','revrs_merged_changes_recent',\n",
    "               'revrs_merged_changes_avg_recent','revrs_abandoned_changes_recent',\n",
    "               'revrs_abandoned_changes_avg_recent','revrs_open_changes_recent',\n",
    "               'revrs_open_changes_avg_recent','revrs_num_review_recent','revrs_num_review_avg_recent',\n",
    "               'revrs_previous_msgs_recent','revrs_previous_msgs_avg_recent','owner_revrs_commons_changes_recent',\n",
    "               'owner_revrs_commons_msgs_avg_recent']\n",
    "    }\n",
    "\n",
    "RQ3_SELECTED_FEATURES = {\n",
    "    'Openstack': ['created_weekday_owner_tz','created_hours_utc','created_hours_owner_tz',\n",
    "                  'betweenness_centrality','core_number','insertions','deletions','num_files_type',\n",
    "                  'sum_complexity','num_binary_files','subject_length','msg_word_count','is_non_fonctional',\n",
    "                  'is_refactoring','is_corrective','is_preventive','has_feature_addition','is_merge',\n",
    "                  'num_files_changes','num_owner_open_changes_recent','owner_age','owner_changes_messages_avg',\n",
    "                  'num_prior_owner_project_changes','num_project_open_changes_recent','revrs_abandoned_changes_recent'],\n",
    "    \n",
    "    'Android': ['created_weekday_owner_tz','created_hours_utc','created_hours_owner_tz','clustering_coefficient',\n",
    "                'insertions','deletions','num_directory','sum_complexity','num_binary_files','subject_length',\n",
    "                'msg_length','is_non_fonctional','is_preventive','is_corrective','has_feature_addition','is_merge',\n",
    "                'files_changes_duration_avg','files_revisions_duration_avg','num_file_changes_for_revrs_avg_recent',\n",
    "                'owner_age','num_owner_open_changes_recent','owner_changes_messages_avg','owner_time_between_message_avg',\n",
    "                'owner_merged_ratio','prior_owner_project_changes_ratio','num_open_changes_recent',\n",
    "                'num_prior_project_changes','revrs_abandoned_changes_avg_recent','revrs_open_changes_recent'],\n",
    "    \n",
    "    'Qt': ['created_weekday_owner_tz','created_hours_utc','eigenvector_centrality','core_number','code_churn',\n",
    "           'deletions','num_files_type','num_programming_languages','sum_complexity','num_binary_files','subject_length',\n",
    "           'msg_word_count','is_non_fonctional','is_refactoring','is_corrective','is_preventive','has_feature_addition',\n",
    "           'is_merge','num_file_changes_for_revrs_max_recent','files_changes_duration_avg','files_revisions_duration_avg',\n",
    "           'files_num_recent_branch_changes','owner_revrs_commons_msgs_avg_recent','num_owner_open_changes_recent',\n",
    "           'owner_changes_messages_avg','owner_time_between_message_avg','owner_merged_ratio','prior_owner_project_changes_ratio',\n",
    "           'num_prior_project_changes','num_open_changes_recent','revrs_abandoned_changes_avg_recent','revrs_open_changes_avg_recent'],\n",
    "    \n",
    "    'Eclipse': ['created_weekday_owner_tz','created_hours_utc','created_hours_owner_tz','betweenness_centrality',\n",
    "                'clustering_coefficient','num_directory','deletions','num_files_type','sum_complexity','num_binary_files',\n",
    "                'subject_length','msg_word_count','is_non_fonctional','is_refactoring','is_corrective','is_preventive',\n",
    "                'has_feature_addition','is_merge','num_file_changes_for_revrs_max_recent','files_changes_duration_avg',\n",
    "                'files_revisions_duration_avg','files_num_recent_branch_changes','owner_age','num_owner_open_changes_recent',\n",
    "                'prior_owner_rate','owner_changes_messages_avg','owner_time_between_message_avg','owner_merged_ratio',\n",
    "                'num_prior_owner_project_changes','prior_owner_project_changes_ratio','num_prior_changes','num_project_open_changes_recent',\n",
    "                'num_open_changes_recent','revrs_open_changes_recent'],\n",
    "    \n",
    "    'Libreoffice': ['created_weekday_utc','created_hours_utc','created_hours_owner_tz','closeness_centrality',\n",
    "                    'clustering_coefficient','code_churn','deletions','modify_entropy','num_programming_languages',\n",
    "                    'sum_complexity','num_binary_files','msg_length','is_non_fonctional','is_refactoring','is_corrective',\n",
    "                    'is_preventive','has_feature_addition','is_merge','num_file_changes_for_revrs_avg_recent',\n",
    "                    'files_changes_duration_avg','files_revisions_duration_avg','files_num_recent_branch_changes',\n",
    "                    'num_owner_open_changes_recent','revrs_abandoned_changes_avg_recent','owner_changes_messages_avg',\n",
    "                    'owner_time_between_message_avg','owner_merged_ratio','num_open_changes_recent',\n",
    "                    'revrs_open_changes_recent']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979d7219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_variables(org) : \n",
    "    global ORGANIZATION\n",
    "    ORGANIZATION = org\n",
    "    \n",
    "    \n",
    "def guess_n_times_mae(X_train,y_train,X_test,y_test,n_guesses = 1000) : \n",
    "    all_guesses = []\n",
    "    all_trues = []\n",
    "    y_test_list = list(y_test)\n",
    "    for n_guess in range(n_guesses): \n",
    "        random_guess_model = RandomGuess()\n",
    "        random_guess_model.fit(X_train,y_train)\n",
    "        guess = random_guess_model.predict(X_test)\n",
    "        all_guesses += guess\n",
    "        all_trues += y_test_list\n",
    "    return mean_absolute_error(all_trues,all_guesses)\n",
    "\n",
    "def SAE(X_train,y_train,y_true,y_pred,random_guess_mae = None,random_guess_runs = 1000) : \n",
    "    if random_guess_mae is None: \n",
    "        random_guess_mae_n_times = guess_n_times_mae(X_train,y_train,y_true,y_true,n_guesses = 1000) \n",
    "    else :\n",
    "        random_guess_mae_n_times = random_guess_mae \n",
    "    \n",
    "    return 1 - (mean_absolute_error(y_true,y_pred)/random_guess_mae_n_times)\n",
    "    \n",
    "def get_data():\n",
    "    all_data =  pd.read_csv(os.path.join(FEATURES_PATH,DATA_VERSION),on_bad_lines='skip')\n",
    "    time_aware_data = pd.read_csv(os.path.join(FEATURES_PATH,DATA_TIME_VERSION),on_bad_lines='skip')\n",
    "    id_col = \"id\"\n",
    "    result = all_data.copy()\n",
    "    for col in result.columns: \n",
    "        if col in time_aware_data and (col != id_col): \n",
    "            result = result.drop(columns=[col])\n",
    "            \n",
    "    return pd.merge(result,time_aware_data,how = 'inner',on='id')\n",
    "\n",
    "def load_full_data(path) : \n",
    "    return pd.read_csv(path)\n",
    "    \n",
    "def filter_data(data,min_delay_hours = 24,max_delay_hours = 14*24,projects=None): \n",
    "    min_hours = min_delay_hours\n",
    "    max_hours = max_delay_hours\n",
    "    result = data.copy()\n",
    "    if not (projects is None) : \n",
    "        result = result[result['project'].isin(projects)]\n",
    "    result = result[(result[REGRESSION_VARIABLE] >= min_hours) & (result[REGRESSION_VARIABLE] <= max_hours) ]\n",
    "    result = boxplot_filtering(result,REGRESSION_VARIABLE)\n",
    "    #result = result.drop_duplicates(subset=[REGRESSION_VARIABLE], keep='first')\n",
    "    return result \n",
    "\n",
    "def select_data_between_dates(data,first_date,second_date): \n",
    "    result = data[(data['closed'] >= first_date) & (data['closed'] < second_date)]\n",
    "    return result.copy()\n",
    "\n",
    "def select_data_before_date(data,date): \n",
    "    result = data[(data['closed'] < date)]\n",
    "    return result.copy()\n",
    "\n",
    "def select_data_after_date(data,date) : \n",
    "    result = data[(data['closed'] >= date)]\n",
    "    return result.copy()\n",
    "\n",
    "def boxplot_filtering(data,col) : \n",
    "    df = data.copy()\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1    #IQR is interquartile range. \n",
    "    filter = (df[col] >= Q1 - 1.5 * IQR) & (df[col] <= Q3 + 1.5 *IQR)\n",
    "    return df.loc[filter] \n",
    "\n",
    "def select_data_by_indicies(data,indicies): \n",
    "    return data.iloc[indicies]\n",
    "\n",
    "def set_data_version(data_version):\n",
    "    global DATA_VERSION \n",
    "    DATA_VERSION = data_version\n",
    "\n",
    "    \n",
    "def compare_two_data_folds(first_fold,second_fold,first_fold_name,second_fold_name):\n",
    "    first_stats = tfdv.generate_statistics_from_dataframe(first_fold)\n",
    "    second_stats = tfdv.generate_statistics_from_dataframe(second_fold)\n",
    "    tfdv.visualize_statistics(lhs_statistics = first_stats,rhs_statistics = second_stats,lhs_name = first_fold_name,rhs_name= second_fold_name)\n",
    "    \n",
    "def apply_IsolationForest_filtering(data,cols,contamination = 0.1) : \n",
    "    result = data.copy()\n",
    "    random_state = np.random.RandomState(42)\n",
    "    model=IsolationForest(n_estimators=1000,max_samples='auto',contamination=contamination,random_state=random_state)\n",
    "    model.fit(result[cols])\n",
    "    result['scores'] = model.decision_function(result[cols])\n",
    "    result['anomaly_score'] = model.predict(result[cols])\n",
    "    return result[result['anomaly_score']== 1]\n",
    "    \n",
    "def apply_log1p(data,cols) : \n",
    "    result = data.copy()\n",
    "    for col in cols: \n",
    "        if (col in ALL_CATEGORICAL_FEATURES): \n",
    "            continue \n",
    "        result[col] = np.log1p(result[col])\n",
    "    return result \n",
    "\n",
    "def apply_quantile_transoformation(df,cols,outcome,transformor = QuantileTransformer(n_quantiles=1000, random_state=0\n",
    "                                                                             ,output_distribution = 'uniform')):\n",
    "    \n",
    "    result = df.copy()\n",
    "    target_transformer = None\n",
    "    for col in cols+[outcome]:\n",
    "        scaler = copy.deepcopy(transformor)\n",
    "        result.loc[:,col]  = scaler.fit_transform(result.loc[:,col].to_numpy().reshape(-1, 1))\n",
    "        if col == outcome :\n",
    "            target_transformer = scaler\n",
    "    return result, target_transformer\n",
    "def apply_data_scaling(data,cols,sklearn_scaler = RobustScaler()):\n",
    "    result = data.copy()\n",
    "    for col in cols:\n",
    "        scaler = copy.deepcopy(sklearn_scaler)\n",
    "        result.loc[:,col]  = scaler.fit_transform(result.loc[:,col].to_numpy().reshape(-1,1))\n",
    "    return result \n",
    "def apply_boxcox_transformation(data,cols) : \n",
    "    df = data.copy() \n",
    "    lambdas = {}\n",
    "    for col in cols : \n",
    "        if not (col in df.columns) : \n",
    "            continue \n",
    "        print('processing ',col)\n",
    "        df[col],fitted_lambda= boxcox(np.array(df[col])+ 1,lmbda=None)\n",
    "        print('done with ',col)\n",
    "        lambdas[col] = fitted_lambda \n",
    "    \n",
    "    return df,lambdas\n",
    "\n",
    "def prepare_data(filtering_function,\n",
    "                 date_1 = pd.to_datetime(\"2015-01-01\"),\n",
    "                 date_2 = None,contamination = 'auto'\n",
    "                ): \n",
    "    FULL_DATA = load_full_data(path=os.path.join(DATA_PATH,f'{ORGANIZATION}.csv'))\n",
    "    FULL_DATA['created'] = FULL_DATA['created'].apply(pd.to_datetime)\n",
    "    FULL_DATA['closed'] = FULL_DATA['closed'].apply(pd.to_datetime)\n",
    "    if not (date_1 is None): \n",
    "        FULL_DATA = select_data_after_date(FULL_DATA,date_1)\n",
    "    if not (date_2 is None) : \n",
    "        FULL_DATA = select_data_before_date(FULL_DATA,date_2)\n",
    "    READY_DATA = filtering_function(FULL_DATA)\n",
    "    if 'change_number' in READY_DATA.columns : \n",
    "        READY_DATA = READY_DATA.sort_values(by= ['change_number'])\n",
    "    else : \n",
    "        READY_DATA = READY_DATA.sort_values(by= ['change_id'])\n",
    "    READY_DATA['code_churn'] = READY_DATA['insertions'] + READY_DATA['deletions']   \n",
    "   \n",
    "    READY_DATA = READY_DATA.dropna()\n",
    "    if not (contamination is None): \n",
    "        print('starting isolation')\n",
    "        READY_DATA = apply_IsolationForest_filtering(READY_DATA,[REGRESSION_VARIABLE],contamination = contamination)\n",
    "        print('isolation done')\n",
    "    return READY_DATA\n",
    "\n",
    "def preprocess(data,features = FEATURES,\n",
    "               outcome = REGRESSION_VARIABLE,\n",
    "               do_scale_data = False, \n",
    "               do_log_features_log_transform = False,\n",
    "               do_boxcox = False,\n",
    "               quantile_transform = True,\n",
    "               do_pca = True,\n",
    "               contamination = 'auto'\n",
    "              ):\n",
    "    data = data\n",
    "    features = features\n",
    "    transformers = {}\n",
    "    if do_boxcox: \n",
    "        print('boxcox started')\n",
    "        data,lambdas = apply_boxcox_transformation(data,[f for f in features if not (f in ALL_CATEGORICAL_FEATURES)] + [outcome])\n",
    "        transformers['boxplot_lambdas'] = lambdas\n",
    "        print('boxcox done')\n",
    "        \n",
    "    if do_log_features_log_transform : \n",
    "        print('log started')\n",
    "        data = apply_log1p(data,[f for f in features if not (f in ALL_CATEGORICAL_FEATURES)] + [outcome])\n",
    "        print('log done')\n",
    "        \n",
    "    if do_scale_data: \n",
    "        print('scaling started')\n",
    "        data = apply_data_scaling(data,[f for f in features if not (f in ALL_CATEGORICAL_FEATURES)] ) \n",
    "        print('scaling done')\n",
    "        \n",
    "    if quantile_transform: \n",
    "        print('quantile transformation started')\n",
    "        data, target_transformer = apply_quantile_transoformation(data,[f for f in features if not (f in ALL_CATEGORICAL_FEATURES)],outcome=outcome)\n",
    "        transformers['target_quantile_transformer'] =  target_transformer\n",
    "        print('quantile transformation done')\n",
    "        \n",
    "    if do_pca: \n",
    "        print('PCA started')\n",
    "        pca = PCA(n_components = 40,whiten =True,svd_solver = 'full')\n",
    "        pca_data = pca.fit_transform(data[[f for f in features if not (f in ALL_CATEGORICAL_FEATURES)]])\n",
    "        n_components = pca_data.shape[1]\n",
    "        pca_data_dataframe = pd.DataFrame(pca_data,columns = [f'component_{i}' for i in range(n_components)])\n",
    "        new_features = list(pca_data_dataframe.columns)\n",
    "        for col in data : \n",
    "            if col in [f for f in features if not (f in ALL_CATEGORICAL_FEATURES)]: \n",
    "                continue \n",
    "            pca_data_dataframe[col] = data[col].values\n",
    "            if col in features: \n",
    "                new_features.append(col)\n",
    "        data = pca_data_dataframe\n",
    "        features=new_features\n",
    "        print('PCA done')\n",
    "    \n",
    "    return data,features, transformers\n",
    "\n",
    "def cross_project_validation(source_data,target_data ,features = FEATURES,\n",
    "                        outcome = REGRESSION_VARIABLE,\n",
    "                        regression_model = RandomForestRegressor(),\n",
    "                        validation_schema = VALIDATION_SCHEMA,  \n",
    "                        do_log_features_log_transform = True,\n",
    "                        do_boxcox=False,\n",
    "                        do_scale_data = True, \n",
    "                        quantile_transform = True,\n",
    "                        do_pca = True,\n",
    "                        contamination = 'auto',\n",
    "                        is_processed = False\n",
    "                       ): \n",
    "\n",
    "    source_data = source_data\n",
    "    target_data = target_data\n",
    "    features = features\n",
    "    regression_model = regression_model\n",
    "    \n",
    "    if not(is_processed) : \n",
    "        source_data,source_features,source_transformers = preprocess(source_data,features = features,\n",
    "               outcome = outcome,\n",
    "               do_scale_data = do_scale_data, \n",
    "               do_log_features_log_transform = do_log_features_log_transform,\n",
    "               do_boxcox = do_boxcox,\n",
    "               quantile_transform = quantile_transform,\n",
    "               do_pca = do_pca,\n",
    "               contamination = contamination\n",
    "              )\n",
    "        \n",
    "        target_data,target_features,target_transformers = preprocess(target_data,features = features,\n",
    "               outcome = outcome,\n",
    "               do_scale_data = do_scale_data, \n",
    "               do_log_features_log_transform = do_log_features_log_transform,\n",
    "               do_boxcox = do_boxcox,\n",
    "               quantile_transform = quantile_transform,\n",
    "               do_pca = do_pca,\n",
    "               contamination = contamination\n",
    "              )\n",
    "        \n",
    "    \n",
    "    X_train = source_data.loc[:,source_features]\n",
    "    y_train = source_data[outcome]\n",
    "    X_test = target_data.loc[:,target_features]\n",
    "    y_test = target_data[outcome]\n",
    "    results = []\n",
    "   \n",
    "    regression_model.fit(X_train,y_train)\n",
    "    y_train_pred = regression_model.predict(X_train)\n",
    "    y_test_pred = regression_model.predict(X_test)\n",
    "    new_row = {}\n",
    "    if quantile_transform :\n",
    "        source_transformer = source_transformers['target_quantile_transformer']\n",
    "        target_transformer = target_transformers['target_quantile_transformer']\n",
    "        y_train_pred = source_transformer.inverse_transform(np.array(y_train_pred).reshape(-1,1))\n",
    "        y_test_pred = target_transformer.inverse_transform(np.array(y_test_pred).reshape(-1,1))\n",
    "        y_train = source_transformer.inverse_transform(np.array(y_train).reshape(-1,1))\n",
    "        y_test = target_transformer.inverse_transform(np.array(y_test).reshape(-1,1))\n",
    "            \n",
    "    if do_log_features_log_transform : \n",
    "        y_train_pred = np.expm1(y_train_pred)\n",
    "        y_test_pred = np.expm1(y_test_pred)\n",
    "        y_train = np.expm1(y_train)\n",
    "        y_test = np.expm1(y_test)\n",
    "\n",
    "    if do_boxcox: \n",
    "        y_train_pred = inv_boxcox(y_train_pred, lambdas[outcome])\n",
    "        y_test_pred = inv_boxcox(y_test_pred, lambdas[outcome])\n",
    "        y_train = inv_boxcox(y_train, lambdas[outcome])\n",
    "        y_test = inv_boxcox(y_test, lambdas[outcome])\n",
    "    new_row.update(\n",
    "        {\n",
    "        \"Train MSE\": mean_squared_error(y_train,y_train_pred),\n",
    "        \"Test MSE\": mean_squared_error(y_test,y_test_pred),\n",
    "        \"Train MAE\": mean_absolute_error(y_train,y_train_pred),\n",
    "        \"Test MAE\": mean_absolute_error(y_test,y_test_pred),\n",
    "        'Train MAPE' : mean_absolute_percentage_error(y_train,y_train_pred),\n",
    "        'Test MAPE' : mean_absolute_percentage_error(y_test,y_test_pred),\n",
    "        'Test SAE':SAE(X_train,y_train,y_test,y_test_pred)\n",
    "    })\n",
    "    results.append(new_row)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "def validate_regression(data,features = FEATURES,\n",
    "                        outcome = REGRESSION_VARIABLE,\n",
    "                        regression_model = RandomForestRegressor(),\n",
    "                        validation_schema = VALIDATION_SCHEMA,  \n",
    "                        do_log_features_log_transform = True,\n",
    "                        do_boxcox=False,\n",
    "                        do_scale_data = True, \n",
    "                        quantile_transform = True,\n",
    "                        do_pca = True,\n",
    "                        contamination = 'auto',\n",
    "                        is_processed = False,\n",
    "                        transformers = None\n",
    "                       ): \n",
    "    \n",
    "    data = data\n",
    "    features = features\n",
    "    regression_model = regression_model\n",
    "    transformers = transformers\n",
    "    if not(is_processed) : \n",
    "        data,features,transformers = preprocess(data,features = features,\n",
    "               outcome = REGRESSION_VARIABLE,\n",
    "               do_scale_data = do_scale_data, \n",
    "               do_log_features_log_transform = do_log_features_log_transform,\n",
    "               do_boxcox = do_boxcox,\n",
    "               quantile_transform = quantile_transform,\n",
    "               do_pca = do_pca,\n",
    "               contamination = 'auto'\n",
    "              )\n",
    "        \n",
    "    X = data.loc[:,features]\n",
    "    y = data[outcome]\n",
    "    results = []\n",
    "    features_importance = {\n",
    "        feature: [] for feature in features \n",
    "    }\n",
    "    for index,(train_index, test_index) in enumerate(validation_schema.split(X)): \n",
    "        X_train = X.iloc[train_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "        X_test = X.iloc[test_index]\n",
    "        y_test = y.iloc[test_index]\n",
    "        start = time.time()\n",
    "        regression_model.fit(X_train,y_train)\n",
    "        fit_time = time.time() - start\n",
    "        y_train_pred = regression_model.predict(X_train)\n",
    "        y_test_pred = regression_model.predict(X_test)\n",
    "        new_row = {}\n",
    "        if quantile_transform : \n",
    "            transformer = transformers['target_quantile_transformer']\n",
    "            y_train_pred = transformer.inverse_transform(np.array(y_train_pred).reshape(-1,1))\n",
    "            y_test_pred = transformer.inverse_transform(np.array(y_test_pred).reshape(-1,1))\n",
    "            y_train = transformer.inverse_transform(np.array(y_train).reshape(-1,1))\n",
    "            y_test = transformer.inverse_transform(np.array(y_test).reshape(-1,1))\n",
    "            \n",
    "        if do_log_features_log_transform : \n",
    "            y_train_pred = np.expm1(y_train_pred)\n",
    "            y_test_pred = np.expm1(y_test_pred)\n",
    "            y_train = np.expm1(y_train)\n",
    "            y_test = np.expm1(y_test)\n",
    "            \n",
    "        if do_boxcox: \n",
    "            y_train_pred = inv_boxcox(y_train_pred, lambdas[outcome])\n",
    "            y_test_pred = inv_boxcox(y_test_pred, lambdas[outcome])\n",
    "            y_train = inv_boxcox(y_train, lambdas[outcome])\n",
    "            y_test = inv_boxcox(y_test, lambdas[outcome])\n",
    "        #print(y_test[:10])\n",
    "        #print(y_test_pred[:10])\n",
    "        \n",
    "            \n",
    "        new_row.update(\n",
    "            {\n",
    "            \"Train MAE\": mean_absolute_error(y_train,y_train_pred),\n",
    "            \"Test MAE\": mean_absolute_error(y_test,y_test_pred),\n",
    "            'Train MAPE' : mean_absolute_percentage_error(y_train,y_train_pred),\n",
    "            'Test MAPE' : mean_absolute_percentage_error(y_test,y_test_pred),\n",
    "            'Test SAE':SAE(X_train,y_train,y_test,y_test_pred),\n",
    "            'fit_time' :fit_time\n",
    "        })\n",
    "        if not(do_pca) : \n",
    "            \n",
    "            if not (hasattr(regression_model, 'feature_importances_')):\n",
    "                print('model does not support gini feature importance')\n",
    "                continue \n",
    "                \n",
    "            for feature, importance in zip(X_train.columns, regression_model.feature_importances_):\n",
    "                features_importance[feature].append(importance) \n",
    "        results.append(new_row)\n",
    "        print(f'Iteration {index + 1} is done.')\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df,features_importance\n",
    "\n",
    "def run_cross_projects(projects,model_name = 'ExtraTrees',model = ExtraTreesRegressor(),\n",
    "                       repetations = 10,results_path='./results/cross_project_validation_results') :\n",
    "    results = []\n",
    "    os.makedirs(results_path,exist_ok= True)\n",
    "    for source_project in projects : \n",
    "        source_results = []\n",
    "        set_global_variables(source_project)\n",
    "        source_data =  prepare_data(lambda data:filter_data(data,min_delay_hours=12,max_delay_hours=24*30,projects = None))\n",
    "        for target_project in projects : \n",
    "            if source_project == target_project :\n",
    "                continue \n",
    "                \n",
    "            print('source:',source_project,'traget:',target_project)\n",
    "            set_global_variables(target_project)\n",
    "            target_data =  prepare_data(lambda data:filter_data(data,min_delay_hours=12,max_delay_hours=24*30,projects = None))\n",
    "            for repetation in range(repetations) :\n",
    "                print('Repetation:',repetation)\n",
    "                result = cross_project_validation(source_data,target_data,contamination='auto')\n",
    "                for index,row in result.iterrows() :\n",
    "                    dict_row = dict(row)\n",
    "                    dict_row['Source project'] = source_project\n",
    "                    dict_row['Target project'] = target_project\n",
    "                    dict_row['repitation'] = repetation\n",
    "                    source_results.append(dict_row)\n",
    "                    results.append(results)\n",
    "        source_results = pd.DataFrame(source_results)\n",
    "        source_results.to_csv(os.path.join(results_path,f'Disc_corss_project_{source_project}.csv'),index=False)\n",
    "    results = pd.DataFrame(results)\n",
    "    results.to_csv(os.path.join(results_path,f'Disc_corss_project_all.csv'),index=False)\n",
    "                \n",
    "\n",
    "def run_RQ1(orgs,models,nb_repetations,features= FEATURES,features_per_org = {\n",
    "    'Libreoffice' : FEATURES,\n",
    "    'Openstack' : FEATURES,\n",
    "    'Android' : FEATURES,\n",
    "    'Eclipse' : FEATURES, \n",
    "    'Qt' : FEATURES\n",
    "},report_feature_importance = False,run_random_guess = True,\n",
    "            results_folder = \"./results/RQ1_results_exp\") : \n",
    "    \n",
    "    os.makedirs(results_folder,exist_ok=True)\n",
    "    all_results = []\n",
    "    for org in orgs:\n",
    "        \n",
    "        print('working on:',org)\n",
    "        set_global_variables(org)\n",
    "        \n",
    "        features = features\n",
    "        if org in features_per_org : \n",
    "            features = features_per_org[org] \n",
    "        org_results = []\n",
    "        \n",
    "        data = prepare_data( lambda data:filter_data(data,min_delay_hours=12,max_delay_hours=24*30,projects = None))\n",
    "        print(data[REGRESSION_VARIABLE].describe())\n",
    "        for model_name, model in models.items():\n",
    "            model_results = []\n",
    "            feature_importances_results = []\n",
    "            print('running model:',model_name)\n",
    "            for repetation in range(nb_repetations):\n",
    "                #if repetation % 20 == 0 :\n",
    "                print('repetation:',repetation)\n",
    "                results,features_importances  = validate_regression(data=data,regression_model=model,\n",
    "                                                do_boxcox = False,do_scale_data=True,\n",
    "                                               do_log_features_log_transform=True,do_pca=not(report_feature_importance),\n",
    "                                               quantile_transform=True,contamination = 'auto',features = features)\n",
    "                \n",
    "                \n",
    "                feature_importance_row = {\n",
    "                    'repitation' : repetation, \n",
    "                    'model' : model_name\n",
    "                }\n",
    "                for index,row in results.iterrows() : \n",
    "                    dict_row = dict(row)\n",
    "                    dict_row['Organization'] = org\n",
    "                    dict_row['model'] = model_name\n",
    "                    dict_row['repitation'] = repetation\n",
    "                    dict_row['fold'] = index\n",
    "                    all_results.append(dict_row)\n",
    "                    org_results.append(dict_row)\n",
    "                    model_results.append(dict_row)\n",
    "                    feature_importance_row['fold'] = index\n",
    "                    \n",
    "                    if report_feature_importance: \n",
    "                        for feature,importances in features_importances.items(): \n",
    "                            new_row = copy.deepcopy(feature_importance_row)\n",
    "                            new_row['feature_name'] = feature\n",
    "                            new_row['importance_value'] = importances[index] \n",
    "                            feature_importances_results.append(new_row)\n",
    "                        \n",
    "            model_results = pd.DataFrame(model_results)\n",
    "            #model_results.to_csv(f'{org}_{model_name}_RQ1.csv',index=False)\n",
    "            if report_feature_importance:\n",
    "                model_features_importances = pd.DataFrame(feature_importances_results)\n",
    "                model_features_importances.to_csv(os.path.join(results_folder,f'{org}_{model_name}_features_importances.csv'),index=False)\n",
    "            \n",
    "        org_results = pd.DataFrame(org_results)\n",
    "        org_results.to_csv(os.path.join(results_folder,f'{org}_RQ1.csv'),index=False)\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "def tune_params(data,org_name,model,model_name,grid) : \n",
    "    all_params = grid.keys()\n",
    "    all_results = []\n",
    "    keys, values = zip(*grid.items())\n",
    "    combinations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    set_global_variables(org_name)\n",
    "    data = prepare_data( lambda data:filter_data(data,min_delay_hours=6,max_delay_hours=24*30,projects = None))\n",
    "    preprocessed_data,final_features = preprocess(data,outcome = REGRESSION_VARIABLE,\n",
    "               do_scale_data = True, \n",
    "               do_log_features_log_transform = True,\n",
    "               do_boxcox = False,\n",
    "               quantile_transform = True,\n",
    "               do_pca = True,\n",
    "               contamination = 'auto')\n",
    "    for combination in combinations_dicts : \n",
    "        new_row = {\n",
    "            'Organization':org_name,\n",
    "            'model' : model_name\n",
    "        }\n",
    "        new_row.update(combination)\n",
    "        model = copy.deepcopy(model)\n",
    "        print(combination)\n",
    "        model.set_params(**combination)\n",
    "        results = validate_regression(preprocessed_data,features = final_features,\n",
    "                        outcome = REGRESSION_VARIABLE,\n",
    "                        regression_model = model,\n",
    "                        validation_schema = VALIDATION_SCHEMA,  \n",
    "                        do_log_features_log_transform = True,\n",
    "                        do_boxcox=False,\n",
    "                        do_IsolationForest_filtering = True,\n",
    "                        do_scale_data = True, \n",
    "                        quantile_transform = True,\n",
    "                        do_pca = True,\n",
    "                        contamination = 'auto',\n",
    "                        is_processed = True\n",
    "                       )\n",
    "            \n",
    "        new_row.update({\n",
    "            'Mean Test SA' : results['Test SAE'].mean(),\n",
    "            'Mean Test MRE' : results['Test MAPE'].mean(),\n",
    "            'Mean Test MAE' : results['Test MAE'].mean()\n",
    "        })\n",
    "        all_results.append(new_row)\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "def run_RQ2(orgs,model = ExtraTreesRegressor(n_jobs=-1),model_name = 'ExtraTrees_dim_validation',dims = DIMS\n",
    "            ,nb_repetitions = 10,results_path = './results/RQ2_results') : \n",
    "    all_results = []\n",
    "    os.makedirs(results_path,exist_ok=True)\n",
    "    for org in orgs : \n",
    "        org_results = []\n",
    "        print('working on:',org)\n",
    "        set_global_variables(org)\n",
    "        data = prepare_data( lambda data:filter_data(data,min_delay_hours=12,max_delay_hours=24*30,projects = None))\n",
    "        print('validating dims with ',model_name)\n",
    "        for dim_name, dim in dims.items() : \n",
    "            print('validating dim ',dim_name)\n",
    "            dim_results = []\n",
    "            for repetation in range(nb_repetitions) : \n",
    "                if repetation % 5 == 0 :\n",
    "                    print('repetation:',repetation)\n",
    "                results,_  = validate_regression(data=data,\n",
    "                                               regression_model=copy.deepcopy(model),do_boxcox = False,do_scale_data=True,\n",
    "                                               do_log_features_log_transform=True,do_pca=False,\n",
    "                                               quantile_transform=True,contamination = 'auto',features = list(set(dim)))\n",
    "                \n",
    "\n",
    "                for index,row in results.iterrows() : \n",
    "                    dict_row = dict(row)\n",
    "                    dict_row['Organization'] = org\n",
    "                    dict_row['model'] = model_name\n",
    "                    dict_row['dim'] = dim_name\n",
    "                    dict_row['fold'] = index\n",
    "                    dict_row['repitation'] = repetation\n",
    "                    dim_results.append(dict_row)\n",
    "                    all_results.append(dict_row)\n",
    "                    org_results.append(dict_row)\n",
    "                    \n",
    "            dim_results = pd.DataFrame(dim_results)\n",
    "            #dim_results.to_csv(f'{org}_{dim_name}_{model_name}_RQ2.csv',index=False)\n",
    "        org_results = pd.DataFrame(org_results)\n",
    "        org_results.to_csv(os.path.join(results_path,f'{org}_RQ2.csv'),index=False)\n",
    "    all_results = pd.DataFrame(all_results)\n",
    "    #all_results.to_csv(f'RQ2_{model_name}.csv',index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150759ba",
   "metadata": {},
   "source": [
    "## Parameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33279f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#may take a long time to run\n",
    "PT_results = \"./PT_results\"\n",
    "for org in ['Android','Qt','Openstack','Libreoffice','Eclipse']:\n",
    "    print('Processing:',org)\n",
    "    set_global_variables(org)\n",
    "    project_results_path = os.path.join(PT_results,org)\n",
    "    os.makedirs(project_results_path,exist_ok = True)\n",
    "    data = prepare_data( lambda data:filter_data(data,min_delay_hours=6,max_delay_hours=24*30,projects = None))\n",
    "    for model_name,model_params in ALL_MODELS.items() : \n",
    "        print('Tunning model:',model_name)\n",
    "        print('Grid:',model_params['grid'])\n",
    "        tunning_results = tune_params(data,org,model_params['default'],model_name,grid=model_params['grid'] )\n",
    "        tunning_results.to_csv(os.path.join(project_results_path,f'{model_name}_tunning_results.csv'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7792307",
   "metadata": {},
   "source": [
    "## RQ1: models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc1002e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#models parameters are choosen based on PT results\n",
    "ml_models_results = run_RQ1(['Android','Libreoffice','Eclipse','Qt','Openstack'],models = {\n",
    "    'ExtreTrees' : ExtraTreesRegressor(n_jobs = -1,max_depth=10,n_estimators=500),\n",
    "    'RandomForest' : RandomForestRegressor(n_jobs = -1,max_depth=10,n_estimators=500),\n",
    "    'AdaBoost' : AdaBoostRegressor(n_estimators=100,learning_rate=0.01,loss='linear'),\n",
    "    'XGBOOST' : xg.XGBRegressor(n_jobs=-1,max_depth = 5,learning_rate=0.01,n_estimators=500),\n",
    "    'DecisionTree': DecisionTreeRegressor(criterion='friedman_mse',max_depth=5,splitter='best'),\n",
    "    'Median' : DummyRegressor(strategy=\"median\"), \n",
    "    'Mean' : DummyRegressor(strategy= 'mean'),   \n",
    "    'RandomGuess':RandomGuess()\n",
    "},nb_repetations=1,report_feature_importance=False,run_random_guess=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69711db2",
   "metadata": {},
   "source": [
    "## RQ2 dimension validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6229c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_RQ2(['Libreoffice','Android','Openstack','Eclipse','Qt'],model=ExtraTreesRegressor(n_jobs = -1),model_name='ExtraTrees',nb_repetitions=1,dims=DIMS,results_path='./results/RQ2_val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d280a",
   "metadata": {},
   "source": [
    "## RQ3 feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd3c03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = run_RQ1(['Libreoffice','Android','Qt','Openstack','Eclipse'],models = {'ExtreTrees_feature_ranking_reduced' : ExtraTreesRegressor()},nb_repetations=1,features_per_org=RQ3_SELECTED_FEATURES,results_folder='./results/RQ3_results_validation',report_feature_importance = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e211a17d",
   "metadata": {},
   "source": [
    "## Cross project validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a9f2a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_cross_projects(['Libreoffice','Android','Eclipse','Openstack','Qt'],model_name = 'ExtraTrees',model = ExtraTreesRegressor(),repetations = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04328d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
